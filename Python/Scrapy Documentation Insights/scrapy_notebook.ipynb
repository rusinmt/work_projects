{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289a48c-5992-4599-89e6-e45bdeeea877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.getLogger('scrapy').propagate = False\n",
    "warnings.filterwarnings('ignore', category=scrapy.exceptions.ScrapyDeprecationWarning)\n",
    "pdf_folder_path = r'C:\\Users\\Mateusz\\Documents\\epu_arch'\n",
    "\n",
    "def extract(filename):\n",
    "    return filename[:9]\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "unique_numbers = set()\n",
    "for pdf_file in pdf_files:\n",
    "    number = extract(pdf_file)\n",
    "    if number:\n",
    "        unique_numbers.add(number)\n",
    "\n",
    "sorted_unique_numbers = sorted(list(unique_numbers))\n",
    "\n",
    "class LinkSpider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "    def start_requests(self):\n",
    "        base_url = 'http://192.168.10.7:7000/synology?q='\n",
    "        for number in tqdm(sorted_unique_numbers):\n",
    "            url = base_url + number\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LinkSpider, self).__init__(*args, **kwargs)\n",
    "        self.visited_urls = set()\n",
    "\n",
    "    def parse(self, response):\n",
    "        if response.url not in self.visited_urls:\n",
    "            self.visited_urls.add(response.url)\n",
    "            link_texts = response.css('a::text').getall()\n",
    "            for link_text in link_texts:\n",
    "                yield {'text': link_text}\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEED_FORMAT': 'jsonlines',\n",
    "    'FEED_URI': 'links.jsonl',\n",
    "    'DUPEFILTER_CLASS': 'scrapy.dupefilters.RFPDupeFilter'\n",
    "})\n",
    "process.crawl(LinkSpider)\n",
    "process.start()\n",
    "\n",
    "results = []\n",
    "with open('links.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "with open(r'C:\\Users\\Mateusz\\Documents\\epu_arch\\arca.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb542a-8825-4194-9f5a-958597a77c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "jp = r\"C:\\Users\\Mateusz\\Documents\\epu_arch\\arca.json\"\n",
    "df = pd.read_json(jp)\n",
    "df = df[['text']].drop_duplicates()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686728d-36b8-4438-8880-27ed606e53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pdf_files = [os.path.splitext(f)[0] for f in os.listdir(r\"C:\\Users\\Mateusz\\Documents\\epu_arch\") if f.endswith('.pdf')]\n",
    "\n",
    "def find_match(pdf_name, df):\n",
    "    match = df[df['text'].str.contains(pdf_name, case=False, na=False)]\n",
    "    return match\n",
    "\n",
    "result = []\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    match = find_match(pdf_file, df)\n",
    "    if not match.empty:\n",
    "        ref = match['text'].values[0]\n",
    "        ref = \"-\".join(ref.split(\"-\")[:4])\n",
    "        result.append(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d0f12-590d-4d29-adb5-078a834f1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(r\"C:\\Users\\Mateusz\\Desktop\\arca.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';')\n",
    "    for ref in result:\n",
    "        writer.writerow([ref])\n",
    "        \n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fba815-49ae-4aa7-84a3-02c44d932ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "result = pd.read_csv(r\"C:\\Users\\Mateusz\\Desktop\\arca.csv\", delimiter=';', encoding='windows-1250', header=None)\n",
    "for _, row in result.iterrows():\n",
    "    result = row.tolist()   \n",
    "    pdf_files = [f for f in os.listdir(r\"C:\\Users\\Mateusz\\Documents\\epu_arch\") if f.endswith('.pdf')]\n",
    "    matches = [ref for ref in result if f'{ref}.pdf' in pdf_files]\n",
    "    for match in matches:\n",
    "        path = f'{match}.pdf'\n",
    "        if path in os.listdir(r\"C:\\Users\\Mateusz\\Documents\\epu_arch\"):\n",
    "            os.remove(os.path.join(r\"C:\\Users\\Mateusz\\Documents\\epu_arch\", path))\n",
    "\n",
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
